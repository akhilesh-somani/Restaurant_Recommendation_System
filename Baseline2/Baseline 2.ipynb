{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on Aug 9, 2016\n",
    "Keras Implementation of Neural Matrix Factorization (NeuMF) recommender model in:\n",
    "He Xiangnan et al. Neural Collaborative Filtering. In WWW 2017.  \n",
    "\n",
    "@author: Xiangnan He (xiangnanhe@gmail.com)\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "#import os\n",
    "#os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, Concatenate, Reshape, Multiply, Flatten, Dropout\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from evaluate import evaluate_model\n",
    "from Dataset import Dataset\n",
    "from time import time\n",
    "import sys\n",
    "import GMF, MLP\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "#################### Arguments ####################\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run NeuMF.\")\n",
    "    parser.add_argument('--path', nargs='?', default='Data/',\n",
    "                        help='Input data path.')\n",
    "    parser.add_argument('--dataset', nargs='?', default='ml-1m',\n",
    "                        help='Choose a dataset.')\n",
    "    parser.add_argument('--epochs', type=int, default=100,\n",
    "                        help='Number of epochs.')\n",
    "    parser.add_argument('--batch_size', type=int, default=256,\n",
    "                        help='Batch size.')\n",
    "    parser.add_argument('--num_factors', type=int, default=8,\n",
    "                        help='Embedding size of MF model.')\n",
    "    parser.add_argument('--layers', nargs='?', default='[64,32,16,8]',\n",
    "                        help=\"MLP layers. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size.\")\n",
    "    parser.add_argument('--reg_mf', type=float, default=0,\n",
    "                        help='Regularization for MF embeddings.')                    \n",
    "    parser.add_argument('--reg_layers', nargs='?', default='[0,0,0,0]',\n",
    "                        help=\"Regularization for each MLP layer. reg_layers[0] is the regularization for embeddings.\")\n",
    "    parser.add_argument('--num_neg', type=int, default=4,\n",
    "                        help='Number of negative instances to pair with a positive instance.')\n",
    "    parser.add_argument('--lr', type=float, default=0.001,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--learner', nargs='?', default='adam',\n",
    "                        help='Specify an optimizer: adagrad, adam, rmsprop, sgd')\n",
    "    parser.add_argument('--verbose', type=int, default=1,\n",
    "                        help='Show performance per X iterations')\n",
    "    parser.add_argument('--out', type=int, default=1,\n",
    "                        help='Whether to save the trained model.')\n",
    "    parser.add_argument('--mf_pretrain', nargs='?', default='',\n",
    "                        help='Specify the pretrain model file for MF part. If empty, no pretrain will be used')\n",
    "    parser.add_argument('--mlp_pretrain', nargs='?', default='',\n",
    "                        help='Specify the pretrain model file for MLP part. If empty, no pretrain will be used')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def init_normal(shape, dtype = None, name=None):\n",
    "    # RandomNormal does not have scale and name\n",
    "    return initializers.RandomNormal(shape)  # scale=0.01, , name=name\n",
    "\n",
    "def get_model(num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0):\n",
    "    assert len(layers) == len(reg_layers)\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    print('Input layer: ',user_input.shape,user_input.dtype)\n",
    "    \n",
    "    # Embedding layer\n",
    "    \"\"\"MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_mf), input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_mf), input_length=1)   \"\"\"\n",
    "    \n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user',\n",
    "                                  embeddings_initializer = 'random_normal', W_regularizer = l2(reg_mf), input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item',\n",
    "                                  embeddings_initializer = 'random_normal', W_regularizer = l2(reg_mf), input_length=1)  \n",
    "\n",
    "    \"\"\"MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = layers[0]/2, name = \"mlp_embedding_user\",\n",
    "                                  init = init_normal, W_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = layers[0]/2, name = 'mlp_embedding_item',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_layers[0]), input_length=1)  \"\"\" \n",
    "    \n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = int(layers[0]/2), name = \"mlp_embedding_user\",\n",
    "                                   embeddings_initializer = 'random_normal', W_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = int(layers[0]/2), name = 'mlp_embedding_item',\n",
    "                                   embeddings_initializer = 'random_normal', W_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "\n",
    "    output = MLP_Embedding_User(user_input)\n",
    "    print(MLP_Embedding_User.weights)\n",
    "    \n",
    "    # MF part\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    mf_vector = Multiply()([mf_user_latent, mf_item_latent]) # element-wise multiply\n",
    "\n",
    "    # MLP part \n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    \n",
    "    mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "    for idx in range(1, num_layer):\n",
    "        layer = Dense(layers[idx], W_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    # Concatenate MF and MLP parts\n",
    "    #mf_vector = Lambda(lambda x: x * alpha)(mf_vector)\n",
    "    #mlp_vector = Lambda(lambda x : x * (1-alpha))(mlp_vector)\n",
    "    predict_vector = Concatenate()([mf_vector, mlp_vector])\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = \"prediction\")(predict_vector)\n",
    "    \n",
    "    model = Model(input=[user_input, item_input], \n",
    "                  output=prediction)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_pretrain_model(model, gmf_model, mlp_model, num_layers):\n",
    "    # MF embeddings\n",
    "    gmf_user_embeddings = gmf_model.get_layer('user_embedding').get_weights()\n",
    "    gmf_item_embeddings = gmf_model.get_layer('item_embedding').get_weights()\n",
    "    model.get_layer('mf_embedding_user').set_weights(gmf_user_embeddings)\n",
    "    model.get_layer('mf_embedding_item').set_weights(gmf_item_embeddings)\n",
    "    \n",
    "    # MLP embeddings\n",
    "    mlp_user_embeddings = mlp_model.get_layer('user_embedding').get_weights()\n",
    "    mlp_item_embeddings = mlp_model.get_layer('item_embedding').get_weights()\n",
    "    model.get_layer('mlp_embedding_user').set_weights(mlp_user_embeddings)\n",
    "    model.get_layer('mlp_embedding_item').set_weights(mlp_item_embeddings)\n",
    "    \n",
    "    # MLP layers\n",
    "    for i in range(1, num_layers):\n",
    "        mlp_layer_weights = mlp_model.get_layer('layer%d' %i).get_weights()\n",
    "        model.get_layer('layer%d' %i).set_weights(mlp_layer_weights)\n",
    "        \n",
    "    # Prediction weights\n",
    "    gmf_prediction = gmf_model.get_layer('prediction').get_weights()\n",
    "    mlp_prediction = mlp_model.get_layer('prediction').get_weights()\n",
    "    new_weights = np.concatenate((gmf_prediction[0], mlp_prediction[0]), axis=0)\n",
    "    new_b = gmf_prediction[1] + mlp_prediction[1]\n",
    "    model.get_layer('prediction').set_weights([0.5*new_weights, 0.5*new_b])    \n",
    "    return model\n",
    "\n",
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    s1 = set(train.keys())\n",
    "    l1 = len(train)\n",
    "    i1=0; verb = 100000;\n",
    "    for (u, i) in train.keys():\n",
    "        i1+=1;\n",
    "        if i1 % verb == 0:\n",
    "            print(i1)\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while (u, j) in s1:\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['NeuMF.py --dataset ml-1m --epochs 20 --batch_size 256 --num_factors 8 --layers [64,32,16,8] --num_neg 4 --lr 0.001 --learner adam --verbose 1 --out 1 --mf_pretrain Pretrain/ml-1m_GMF_8_1501651698.h5 --mlp_pretrain Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuMF arguments: Namespace(batch_size=256, dataset='ml-1m', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', num_factors=8, num_neg=4, out=1, path='Data/', reg_layers='[0,0,0,0]', reg_mf=0, verbose=1) \n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "num_epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "mf_dim = args.num_factors\n",
    "layers = eval(args.layers)\n",
    "reg_mf = args.reg_mf\n",
    "reg_layers = eval(args.reg_layers)\n",
    "num_negatives = args.num_neg\n",
    "learning_rate = args.lr\n",
    "learner = args.learner\n",
    "verbose = args.verbose\n",
    "mf_pretrain = args.mf_pretrain\n",
    "mlp_pretrain = args.mlp_pretrain\n",
    "\n",
    "topK = 10\n",
    "evaluation_threads = 1#mp.cpu_count()\n",
    "print(\"NeuMF arguments: %s \" %(args))\n",
    "model_out_file = 'Pretrain/%s_NeuMF_%d_%s_%d.h5' %(args.dataset, mf_dim, args.layers, time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data done [16.1 s]. #user=6040, #item=3706, #train=994169, #test=6040\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "t1 = time()\n",
    "dataset = Dataset(args.path + args.dataset)\n",
    "train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "num_users, num_items = train.shape\n",
    "print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
    "      %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 3706 8 [64, 32, 16, 8] [0, 0, 0, 0] 0\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(num_users, num_items, mf_dim, layers, reg_layers, reg_mf)\n",
    "print(type(layers[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer:  (None, 1) <dtype: 'int32'>\n",
      "[<tf.Variable 'mlp_embedding_user/embeddings:0' shape=(6040, 32) dtype=float32, numpy=\n",
      "array([[ 0.01188685,  0.0143198 , -0.03366586, ...,  0.07395183,\n",
      "        -0.01283339,  0.04963186],\n",
      "       [ 0.02442405,  0.07235339,  0.05449707, ...,  0.02182658,\n",
      "        -0.02979731,  0.09355062],\n",
      "       [-0.00210632, -0.07061072,  0.01770441, ...,  0.05463504,\n",
      "        -0.04244259,  0.01614661],\n",
      "       ...,\n",
      "       [-0.01019938,  0.03161612, -0.01262934, ..., -0.05206285,\n",
      "         0.13254254,  0.05696766],\n",
      "       [ 0.05083417, -0.01149529,  0.05168714, ...,  0.03490419,\n",
      "         0.00466869,  0.03623126],\n",
      "       [ 0.02138065,  0.0425919 ,  0.03691208, ..., -0.03126816,\n",
      "        -0.00440133,  0.05828046]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NeuMF.py --dataset ml-1m --epochs 20 --batch_size 256 --num_factors 8 --layers [64,32,16,8] --num_neg 4 --lr 0.001 --learner adam --verbose 1 --out 1 --mf_pretrain Pretrain/ml-1m_GMF_8_1501651698.h5 --mlp_pretrain Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5:87: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=6040, output_dim=8, name=\"mf_embedding_user\", embeddings_initializer=\"random_normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "NeuMF.py --dataset ml-1m --epochs 20 --batch_size 256 --num_factors 8 --layers [64,32,16,8] --num_neg 4 --lr 0.001 --learner adam --verbose 1 --out 1 --mf_pretrain Pretrain/ml-1m_GMF_8_1501651698.h5 --mlp_pretrain Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5:89: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=3706, output_dim=8, name=\"mf_embedding_item\", embeddings_initializer=\"random_normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "NeuMF.py --dataset ml-1m --epochs 20 --batch_size 256 --num_factors 8 --layers [64,32,16,8] --num_neg 4 --lr 0.001 --learner adam --verbose 1 --out 1 --mf_pretrain Pretrain/ml-1m_GMF_8_1501651698.h5 --mlp_pretrain Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5:97: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=6040, output_dim=32, name=\"mlp_embedding_user\", embeddings_initializer=\"random_normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "NeuMF.py --dataset ml-1m --epochs 20 --batch_size 256 --num_factors 8 --layers [64,32,16,8] --num_neg 4 --lr 0.001 --learner adam --verbose 1 --out 1 --mf_pretrain Pretrain/ml-1m_GMF_8_1501651698.h5 --mlp_pretrain Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5:99: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=3706, output_dim=32, name=\"mlp_embedding_item\", embeddings_initializer=\"random_normal\", input_length=1, embeddings_regularizer=<keras.reg...)`\n",
      "NeuMF.py --dataset ml-1m --epochs 20 --batch_size 256 --num_factors 8 --layers [64,32,16,8] --num_neg 4 --lr 0.001 --learner adam --verbose 1 --out 1 --mf_pretrain Pretrain/ml-1m_GMF_8_1501651698.h5 --mlp_pretrain Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5:116: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, activation=\"relu\", name=\"layer1\", kernel_regularizer=<keras.reg...)`\n",
      "NeuMF.py --dataset ml-1m --epochs 20 --batch_size 256 --num_factors 8 --layers [64,32,16,8] --num_neg 4 --lr 0.001 --learner adam --verbose 1 --out 1 --mf_pretrain Pretrain/ml-1m_GMF_8_1501651698.h5 --mlp_pretrain Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5:116: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, activation=\"relu\", name=\"layer2\", kernel_regularizer=<keras.reg...)`\n",
      "NeuMF.py --dataset ml-1m --epochs 20 --batch_size 256 --num_factors 8 --layers [64,32,16,8] --num_neg 4 --lr 0.001 --learner adam --verbose 1 --out 1 --mf_pretrain Pretrain/ml-1m_GMF_8_1501651698.h5 --mlp_pretrain Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5:116: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(8, activation=\"relu\", name=\"layer3\", kernel_regularizer=<keras.reg...)`\n",
      "NeuMF.py --dataset ml-1m --epochs 20 --batch_size 256 --num_factors 8 --layers [64,32,16,8] --num_neg 4 --lr 0.001 --learner adam --verbose 1 --out 1 --mf_pretrain Pretrain/ml-1m_GMF_8_1501651698.h5 --mlp_pretrain Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5:125: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", name=\"prediction\", kernel_initializer=\"lecun_uniform\")`\n",
      "NeuMF.py --dataset ml-1m --epochs 20 --batch_size 256 --num_factors 8 --layers [64,32,16,8] --num_neg 4 --lr 0.001 --learner adam --verbose 1 --out 1 --mf_pretrain Pretrain/ml-1m_GMF_8_1501651698.h5 --mlp_pretrain Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5:128: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"pr...)`\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = get_model(num_users, num_items, mf_dim, layers, reg_layers, reg_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if learner.lower() == \"adagrad\": \n",
    "    model.compile(optimizer=Adagrad(lr=learning_rate), loss='binary_crossentropy')\n",
    "elif learner.lower() == \"rmsprop\":\n",
    "    model.compile(optimizer=RMSprop(lr=learning_rate), loss='binary_crossentropy')\n",
    "elif learner.lower() == \"adam\":\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
    "else:\n",
    "    model.compile(optimizer=SGD(lr=learning_rate), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrain model\n",
    "if mf_pretrain != '' and mlp_pretrain != '':\n",
    "    gmf_model = GMF.get_model(num_users,num_items,mf_dim)\n",
    "    gmf_model.load_weights(mf_pretrain)\n",
    "    mlp_model = MLP.get_model(num_users,num_items, layers, reg_layers)\n",
    "    mlp_model.load_weights(mlp_pretrain)\n",
    "    model = load_pretrain_model(model, gmf_model, mlp_model, len(layers))\n",
    "    print(\"Load pretrained GMF (%s) and MLP (%s) models done. \" %(mf_pretrain, mlp_pretrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: HR = 0.1022, NDCG = 0.0473\n"
     ]
    }
   ],
   "source": [
    "# Init performance\n",
    "(hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print('Init: HR = %.4f, NDCG = %.4f' % (hr, ndcg))\n",
    "best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
    "if args.out > 0:\n",
    "    model.save_weights(model_out_file, overwrite=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "Finish generating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NeuMF.py --dataset ml-1m --epochs 20 --batch_size 256 --num_factors 8 --layers [64,32,16,8] --num_neg 4 --lr 0.001 --learner adam --verbose 1 --out 1 --mf_pretrain Pretrain/ml-1m_GMF_8_1501651698.h5 --mlp_pretrain Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "D:\\Anaconda3\\envs\\CS412-project\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 [69.4 s]: HR = 0.5843, NDCG = 0.3288, loss = 0.3246 [4.4 s]\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "Finish generating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NeuMF.py --dataset ml-1m --epochs 20 --batch_size 256 --num_factors 8 --layers [64,32,16,8] --num_neg 4 --lr 0.001 --learner adam --verbose 1 --out 1 --mf_pretrain Pretrain/ml-1m_GMF_8_1501651698.h5 --mlp_pretrain Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 [72.0 s]: HR = 0.6293, NDCG = 0.3644, loss = 0.2770 [4.1 s]\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "Finish generating\n",
      "Iteration 2 [73.5 s]: HR = 0.6533, NDCG = 0.3812, loss = 0.2643 [4.3 s]\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "Finish generating\n",
      "Iteration 3 [76.5 s]: HR = 0.6606, NDCG = 0.3908, loss = 0.2581 [4.9 s]\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "Finish generating\n",
      "Iteration 4 [78.2 s]: HR = 0.6656, NDCG = 0.3918, loss = 0.2542 [4.9 s]\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "Finish generating\n",
      "Iteration 5 [80.5 s]: HR = 0.6690, NDCG = 0.3981, loss = 0.2510 [4.8 s]\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "Finish generating\n",
      "Iteration 6 [83.8 s]: HR = 0.6687, NDCG = 0.3958, loss = 0.2483 [5.2 s]\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "Finish generating\n",
      "Iteration 7 [86.4 s]: HR = 0.6765, NDCG = 0.4021, loss = 0.2457 [7.0 s]\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "for epoch in range(num_epochs):\n",
    "    t1 = time()\n",
    "    # Generate training instances\n",
    "    user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "    print('Finish generating')\n",
    "    \n",
    "    # Training\n",
    "    hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                     np.array(labels), # labels \n",
    "                     batch_size=batch_size, nb_epoch=1, verbose=0, shuffle=True)\n",
    "    t2 = time()\n",
    "\n",
    "    # Evaluation\n",
    "    if epoch % verbose == 0:\n",
    "        (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "        hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "        print('Iteration %d [%.1f s]: HR = %.4f, NDCG = %.4f, loss = %.4f [%.1f s]' \n",
    "              % (epoch,  t2-t1, hr, ndcg, loss, time()-t2))\n",
    "        if hr > best_hr:\n",
    "            best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "            if args.out > 0:\n",
    "                model.save_weights(model_out_file, overwrite=True)\n",
    "\n",
    "print(\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %(best_iter, best_hr, best_ndcg))\n",
    "if args.out > 0:\n",
    "    print(\"The best NeuMF model is saved to %s\" %(model_out_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
